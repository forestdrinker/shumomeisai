# Step 1｜V2.0 新逻辑库与战略辩护（New Logic Library & Strategic Defense）

下面按题目四个 Task（=Q1–Q4）逐一给出：Primary Model（主模型）、Baseline Model（对照基线），以及一段可直接写进论文的Strategic Defense（战略辩护）。整体裁决与修改理由以你给的“最终修改意见”为准：Task1 主线升级为分赛季并行 NUTS MCMC、验证升级为 LOSO+Temporal+Coverage(+轻量SBC)，Task3 强制 LMM锚定 + GBDT需bootstrap置信度，Task4 强制 Pareto + 多目标BO精炼。

> Chatgpt 修改意见 最终版

---

## Task 1（Q1）反推观众票：Vote Inference + Certainty

### Primary Model（V2 主模型）
**分赛季并行的贝叶斯隐变量反演模型：NUTS MCMC 主线（VI 仅作 warm start/回退）**
- **建模对象**：每季内，每周每个组合的人气潜变量 $u_{i,t}$ 与票份额 $v_{i,t}$（满足 $v_{i,t}\ge 0,\ \sum_i v_{i,t}=1$）。
- **时序结构**：用“人气周际平滑/随机游走”刻画 $u_{i,t}$ 的合理连续性（由 $\sigma_u^2$ 控制波动强度）。
- **决策机制**：用题面 Rank/Percent (+Save) 规则把公开的评委分 $S_{i,t}$ 与未知票份额 $v_{i,t}$ 合成周淘汰机制（用 soft-rank/soft-likelihood 版本保证可微与稳定，$\kappa$ 控制 soft-rank 温度，$\lambda$ 控制“硬淘汰”尖锐度）。
- **推断方式**：每个赛季独立建模并行跑 NUTS MCMC，输出 $u,v$ 的后验分布与 CI；对混合困难的赛季，允许 Full-rank Gaussian VI 做 warm start，再回到 MCMC。

> Chatgpt 修改意见 最终版
> 这对应你们逻辑库中对 Task1“逆问题/隐变量反演 + 不确定性输出”的定位，并将最终决策落实为“分赛季并行 NUTS”。
> 逻辑库

### Baseline Model（对照基线）
**每周独立的 Hard-constraint 可行域解：CSP/LP/最大熵单点票份额（无跨周平滑）**
- 在每一周单独求一组 $v_{i,t}$，只要求“淘汰者在该周规则下综合得分最低”（硬约束），不给时间连续性。
- 输出通常是单点解（或少量可行解采样），不强调后验校准，仅用于：
  1. 证明我们不是“凭空拟合”；
  2. 给软模型/贝叶斯模型提供 sanity check。
  3. 逻辑库

### Strategic Defense（战略辩护，可直接写进论文）
我们在 V2 中将 Task1 的主推断从“VI 主线”升级为“分赛季并行 NUTS MCMC”，核心原因是题目明确要求对 fan votes 的 certainty 给出可检验量化，而变分推断在复杂后验（多峰、高相关、可识别性弱）下存在系统性低估后验方差的风险，会导致可信区间偏窄、结论“盲目自信”。因此，在工程资源允许的前提下，我们用 MCMC 作为主线以获得更可靠的不确定性刻画；同时为避免将 34 季联合成超高维巨模型引发收敛灾难，我们采用“分赛季独立估计”把复杂度切分为可控的并行子问题，并在每季输出 $\widehat{R}$、ESS 与 trace 的自动化诊断；当个别赛季出现混合困难时，VI 被严格降级为 初始化/故障回退（full-rank warm start），而非主结论来源，从而在“严谨上限”与“工程可控”之间取得最优平衡。

> Chatgpt 修改意见 最终版

---

## Task 2（Q2）规则对比与争议人物：Counterfactual Replay + Uncertainty Propagation

### Primary Model（V2 主模型）
**不确定性传播的赛季回放引擎：Posterior-driven Monte Carlo Replay（Rank vs Percent vs +Save）**
- **输入来自 Task1**：$v_{i,t}$ 的后验样本（而非点估计）。
- **对每个后验样本，在同一赛季下分别应用**：
  1. Rank 合并
  2. Percent 合并
  3. Rank/Percent + Judges’ Save（bottom-two + 评委二选一，可用 $\gamma$ 控制偏好强度的概率化版本）
- **输出是结果分布**（如冠军改变概率、Top3 改变概率、每周淘汰差异概率、争议人物“翻盘概率/淘汰周分布”），把 Task1 的 uncertainty 一路传递到制度结论。

### Baseline Model（对照基线）
**点估计回放（无不确定性传播）**
- 用 Task1 的 baseline（Hard-constraint 单点 $v$）或后验均值当作“真票”，对规则做一次性回放。
- 只输出单一结果对比（差异周数/冠军是否改变），不给概率、不给区间。
- 逻辑库

### Strategic Defense（战略辩护）
Task2 的核心不是“重放一次历史”，而是回答“两种合并方式在各季是否系统性偏向观众/评委”以及“争议人物在不同制度下会怎样”。由于题面已明确 fan votes 不可观测且多解，若仅用点估计票数做反事实回放，会把 Task1 的不确定性人为压扁成单一路径，导致制度差异结论对某个任意可行解过度敏感。V2 因此采用“后验采样驱动的 Monte Carlo 回放”，把“票的分布”传播到“结果的分布”，用“冠军改变概率/Top3 改变概率/争议人物翻盘概率”等概率化指标直接回应题目对 consistency 与 certainty 的要求；同时把 Judges’ Save 视为分段机制（先连续确定 bottom-two，再离散二选一），以参数化方式纳入并可做敏感性分析，从而使对 Season 28 机制变化的假设既可解释、又可检验。

---

## Task 3（Q3）舞伴与明星特征归因：Attribution with Statistical Confidence

### Primary Model（V2 主模型）
**“解释锚定 + 非线性补强”的双轨体系：LMM（主口径） + GBDT（bootstrap SHAP CI）**
- **主口径（可引用效应）：线性混合效应模型 LMM**
  - 分别对“评委表现”（如 $y_J$）与“观众支持”（如 $y_F$ 或 Task1 推断的 $v$）建模；
  - 至少包含舞伴随机效应 $(1|\text{partner})$ 与赛季随机效应 $(1|\text{season})$，并纳入明星年龄、行业、进程周次等固定效应。
- **补强（捕捉非线性与交互）：GBDT（XGBoost/LightGBM）**
  - Group CV by season 控制跨季泛化；
  - block/bootstrap 重采样训练得到性能分布；
  - 对 SHAP 重要性做 bootstrap 置信区间，避免“SHAP 无显著性”的评委质疑。
  > Chatgpt 修改意见 最终版
- **CS 亮点特征**：明星–舞伴二部图的中心性/Node2Vec 嵌入作为高阶特征输入（用于解释“带飞结构”）。
- 逻辑库

### Baseline Model（对照基线）
**单一统计基线：LMM/线性回归（无嵌入、弱特征）**
- 只用基础明星特征与简单周次特征，不加入网络嵌入；
- 或把舞伴当固定效应（更弱的层级建模），用于对比“随机效应结构”的必要性。
- 逻辑库

### Strategic Defense（战略辩护）
V2 在 Task3 的关键修改是：明确区分“可解释效应”与“高性能预测”，将主结论锚定在带 CI 的 LMM 上，同时允许 GBDT 作为非线性洞见补强，但强制加入 跨季分组验证与 bootstrap 置信度。这一调整直接回应了红队批评：在样本量有限且层级结构明显（舞伴跨季复用、赛季差异显著）的场景下，纯 GBDT + 嵌入容易被质疑过拟合与不可解释。LMM 通过舞伴随机效应给出可引用的“带飞系数”及其可信区间，确保论文主张具备统计可辩护性；而 GBDT 在严格的 Group-CV 与 bootstrap 下提供稳健的预测提升与特征交互解释，并为 SHAP 配置区间，从而把“机器学习解释”从展示性图形升级为具有不确定性刻画的证据链。

---

## Task 4（Q4）新赛制设计：Multi-objective Mechanism Design with Robustness

### Primary Model（V2 主模型）
**多目标优化 + Pareto 前沿 + 两阶段搜索（LHS → 多目标 BO）**
- **参数化规则族**：$\theta=(a,b,\eta,\gamma,\text{save\_flag},\dots)$，允许：
  - 周权重 $w_t$（可用 logistic 曲线控制随周变化的“评委权重/观众权重”）；
  - Rank/Percent 组合方式；
  - 是否启用 Judges’ Save 及其强度参数。
- **目标函数**：至少包含观众对齐 $Obj_F$、评委对齐 $Obj_J$、戏剧性 $Obj_D$、鲁棒性 $Obj_R$（抗刷票/扰动稳定）。
  > Chatgpt 修改意见 最终版
- **搜索策略**：
  1. 随机/拉丁超立方粗搜，快速铺出 Pareto 形状；
  2. 在 Pareto 邻域用 **多目标 Bayesian Optimization（qEHVI 或 TPE）**精炼前沿，给出“膝点推荐 + 偏观众/偏技术备选”。
  > Chatgpt 修改意见 最终版

### Baseline Model（对照基线）
**固定权重的单规则：50/50 线性加权（可选固定 Save），不做多目标搜索**
- 作为节目方常见可执行基线：简单、可复现、无优化器依赖；
- 也可用“仅随机搜索”作为更弱对照，凸显 BO 的效率优势。
- 逻辑库

### Strategic Defense（战略辩护）
Task4 本质是机制设计的权衡问题：公平/参与/戏剧性/鲁棒性天然冲突，单一加权和往往隐含主观权重且容易被质疑“调参拍脑袋”。V2 采用 Pareto 前沿作为制度推荐的核心证据结构，用四目标把“更偏观众/更偏技术/更抗操纵/更有戏剧性”的制度取舍显式化；并用两阶段搜索保证工程可控：先用 LHS/随机粗搜得到全局形状，再在前沿附近用多目标 BO 高效精炼，避免网格爆炸同时提高前沿质量。与此同时，鲁棒性目标与压力测试把“抗刷票”从口头承诺变成可量化曲线/指标，使最终推荐不仅能解释“为什么好”，还能回答“在扰动下是否仍好”。

### Step 1 输出小结（你可以用作“V2 总述”一段）
V2 的核心升级是把整条链路从“点估计 + 静态对比”提升为“分赛季贝叶斯后验分布 + 不确定性传播回放 + 统计置信度归因 + Pareto 机制设计”，并用可执行的验证/诊断体系把 certainty 从叙述性口径变成可检验证据（收敛诊断、OOS 验证、coverage、轻量 SBC）。

> Chatgpt 修改意见 最终版

